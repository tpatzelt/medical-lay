{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:38:40.390448348Z",
     "start_time": "2023-07-06T08:38:38.929076621Z"
    }
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "from config import TLCPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with jsonlines.open(TLCPaths.project_data_path.joinpath(\n",
    "            \"TLC-validation-second-round_dump.jsonl\")) as reader:\n",
    "        for obj in reader:\n",
    "            yield obj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:38:40.400303191Z",
     "start_time": "2023-07-06T08:38:40.392156098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "annotations = list(get_data())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:38:40.424470128Z",
     "start_time": "2023-07-06T08:38:40.396575554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "53.0"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations) / 2  # in the ends its 832"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:38:40.483398933Z",
     "start_time": "2023-07-06T08:38:40.426822216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations1 = {}\n",
    "id1 = 'TLC-validation-second-round-alon'\n",
    "annotations2 = {}\n",
    "id2 = 'TLC-validation-second-round-selin'\n",
    "for annotation in annotations:\n",
    "    name = annotation[\"_input_hash\"]\n",
    "    a_id = annotation[\"_annotator_id\"]\n",
    "    if a_id == id1:\n",
    "        annotations1[name] = annotation\n",
    "    elif a_id == id2:\n",
    "        annotations2[name] = annotation\n",
    "    else:\n",
    "        print(\"unknown annotator id\", a_id)#%%\n",
    "import jsonlines\n",
    "\n",
    "from config import TLCPaths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with jsonlines.open(TLCPaths.project_data_path.joinpath(\n",
    "            \"TLC-validation-second-round_dump.jsonl\")) as reader:\n",
    "        for obj in reader:\n",
    "            yield obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations = list(get_data())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(annotations) / 2  # in the ends its 832"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations1 = {}\n",
    "id1 = 'TLC-validation-second-round-alon'\n",
    "annotations2 = {}\n",
    "id2 = 'TLC-validation-second-round-selin'\n",
    "for annotation in annotations:\n",
    "    name = annotation[\"_input_hash\"]\n",
    "    a_id = annotation[\"_annotator_id\"]\n",
    "    if a_id == id1:\n",
    "        annotations1[name] = annotation\n",
    "    elif a_id == id2:\n",
    "        annotations2[name] = annotation\n",
    "    else:\n",
    "        print(\"unknown annotator id\", a_id)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "same = []\n",
    "diff = []\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == ann2[\"answer\"]:\n",
    "        same.append(name)\n",
    "    else:\n",
    "        diff.append(name)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(same), len(diff))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# treat one ignore as same with the other answer and treat both ignore as both rejected\n",
    "same = []\n",
    "diff = []\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == \"ignore\" and ann2[\"answer\"] == \"ignore\":\n",
    "        ann2['answer'] = \"reject\"\n",
    "        ann1['answer'] = \"reject\"\n",
    "        same.append(name)\n",
    "    elif ann1[\"answer\"] == ann2[\"answer\"]:\n",
    "        same.append(name)\n",
    "    elif ann1[\"answer\"] == \"ignore\" and not ann2[\"answer\"] == \"ignore\":\n",
    "        ann1['answer'] = ann2['answer']\n",
    "        same.append(name)\n",
    "    elif ann2[\"answer\"] == \"ignore\" and not ann1[\"answer\"] == \"ignore\":\n",
    "        ann2['answer'] = ann1['answer']\n",
    "        same.append(name)\n",
    "\n",
    "    else:\n",
    "        diff.append(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(same), len(diff))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get annotations that differ or both rejected\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == \"reject\" or ann2[\"answer\"] == \"reject\":\n",
    "        print(ann1[\"_input_hash\"])\n",
    "# 3 both rejected and 7 not same answer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "not_accepted1 = []\n",
    "accepted1 = []\n",
    "for name, ann in annotations1.items():\n",
    "    if ann[\"answer\"] != \"accept\":\n",
    "        not_accepted1.append(name)\n",
    "    else:\n",
    "        accepted1.append(name)\n",
    "print(\"not accepted: \",len(not_accepted1), \"accepted: \", len(accepted1))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "not_accepted2 = []\n",
    "accepted2 = []\n",
    "for name, ann in annotations2.items():\n",
    "    if ann[\"answer\"] != \"accept\":\n",
    "        not_accepted2.append(name)\n",
    "    else:\n",
    "        accepted2.append(name)\n",
    "print(\"not accepted: \",len(not_accepted2), \"accepted: \", len(accepted2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"both accepted: \", len(set(accepted1).intersection(set(accepted2))))\n",
    "print(\"both discarded: \", len(set(not_accepted1).intersection(set(not_accepted2))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"accepted1 but not accepted2: \", len(set(accepted1).difference(set(accepted2))))\n",
    "print(\"accepted2 but not accepted1: \", len(set(accepted2).difference(set(accepted1))))\n",
    "\n",
    "print(\"not_accepted1 but not_accepted2: \", len(set(not_accepted1).difference(set(not_accepted2))))\n",
    "print(\"not_accepted2 but not_accepted1: \", len(set(not_accepted2).difference(set(not_accepted1))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name in diff:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    print(\"Answers: \", ann1[\"answer\"],\"-\", ann2[\"answer\"])\n",
    "    start = ann1[\"spans\"][0][\"start\"]\n",
    "    end = ann1[\"spans\"][0][\"end\"]\n",
    "    print(ann1[\"text\"][start:end], \"-> \", ann1['html'])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diff1 = [annotations1[name] for name in diff]\n",
    "diff2 = [annotations2[name] for name in diff]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diff1[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "annotator agreement\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate kappa score given that we treat ignore as same with the other answer and treat both ignore as both rejected\n",
    "a1, a2 = [], []\n",
    "for name in annotations1:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    a1.append(ann1[\"answer\"])\n",
    "    a2.append(ann2[\"answer\"])\n",
    "    \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(a1, a2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(a1, a2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(a2, a1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "def parse_classification_report(clfreport):\n",
    "    \"\"\"\n",
    "    Parse a sklearn classification report into a dict keyed by class name\n",
    "    and containing a tuple (precision, recall, fscore, support) for each class\n",
    "    \"\"\"\n",
    "    lines = clfreport.split('\\n')\n",
    "    # Remove empty lines\n",
    "    lines = list(filter(lambda l: not len(l.strip()) == 0, lines))\n",
    "    lines=[s for s in lines if 'accuracy' not in s]\n",
    "\n",
    "    # Starts with a header, then score for each class and finally an average\n",
    "    header = lines[0]\n",
    "    cls_lines = lines[1:-1]\n",
    "    avg_line = lines[-1]\n",
    "\n",
    "    assert header.split() == ['precision', 'recall', 'f1-score', 'support']\n",
    "    assert avg_line.split()[1] == 'avg'\n",
    "\n",
    "    # We cannot simply use split because class names can have spaces. So instead\n",
    "    # figure the width of the class field by looking at the indentation of the\n",
    "    # precision header\n",
    "    cls_field_width = len(header) - len(header.lstrip())\n",
    "    # Now, collect all the class names and score in a dict\n",
    "    def parse_line(l):\n",
    "        \"\"\"Parse a line of classification_report\"\"\"\n",
    "        cls_name = l[:cls_field_width].strip()\n",
    "        precision, recall, fscore, support = l[cls_field_width:].split()\n",
    "        precision = float(precision)\n",
    "        recall = float(recall)\n",
    "        fscore = float(fscore)\n",
    "        support = int(support)\n",
    "        return (cls_name, precision, recall, fscore, support)\n",
    "\n",
    "    data = collections.OrderedDict()\n",
    "    for l in cls_lines:\n",
    "        ret = parse_line(l)\n",
    "        cls_name = ret[0]\n",
    "        scores = ret[1:]\n",
    "        data[cls_name] = scores\n",
    "\n",
    "    # average\n",
    "    data['avg'] = parse_line(avg_line)[1:]\n",
    "\n",
    "    return data\n",
    "\n",
    "def report_to_latex_table(data):\n",
    "    avg_split = False\n",
    "    out = \"\"\n",
    "    out += \"\\\\begin{table}\\n\"\n",
    "    out += \"\\\\caption{Latex Table from Classification Report}\\n\"\n",
    "    out += \"\\\\label{table:classification:report}\\n\"\n",
    "    out += \"\\\\centering\\n\"\n",
    "    out += \"\\\\begin{tabular}{c | c c c r}\\n\"\n",
    "    out += \"Class & Precision & Recall & F-score & Support\\\\\\\\\\n\"\n",
    "    out += \"\\midrule\\n\"\n",
    "    for cls, scores in data.items():\n",
    "        if 'micro' in cls:\n",
    "            out += \"\\\\midrule\\n\"\n",
    "        out += cls + \" & \" + \" & \".join([str(s) for s in scores])\n",
    "        out += \"\\\\\\\\\\n\"\n",
    "    out += \"\\\\end{tabular}\\n\"\n",
    "    out += \"\\\\end{table}\"\n",
    "    return out\n",
    "\n",
    "print(report_to_latex_table(parse_classification_report(classification_report(a1, a2))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "same = []\n",
    "diff = []\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == ann2[\"answer\"]:\n",
    "        same.append(name)\n",
    "    else:\n",
    "        diff.append(name)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:39:09.482334892Z",
     "start_time": "2023-07-06T08:39:09.469993269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 10\n"
     ]
    }
   ],
   "source": [
    "print(len(same), len(diff))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:39:10.823387615Z",
     "start_time": "2023-07-06T08:39:10.798109564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# treat one ignore as same with the other answer and treat both ignore as both rejected\n",
    "same = []\n",
    "diff = []\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == \"ignore\" and ann2[\"answer\"] == \"ignore\":\n",
    "        ann2['answer'] = \"reject\"\n",
    "        ann1['answer'] = \"reject\"\n",
    "        same.append(name)\n",
    "    elif ann1[\"answer\"] == ann2[\"answer\"]:\n",
    "        same.append(name)\n",
    "    elif ann1[\"answer\"] == \"ignore\" and not ann2[\"answer\"] == \"ignore\":\n",
    "        ann1['answer'] = ann2['answer']\n",
    "        same.append(name)\n",
    "    elif ann2[\"answer\"] == \"ignore\" and not ann1[\"answer\"] == \"ignore\":\n",
    "        ann2['answer'] = ann1['answer']\n",
    "        same.append(name)\n",
    "\n",
    "    else:\n",
    "        diff.append(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:30.441312964Z",
     "start_time": "2023-07-06T08:42:30.393763482Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 7\n"
     ]
    }
   ],
   "source": [
    "print(len(same), len(diff))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:30.758401300Z",
     "start_time": "2023-07-06T08:42:30.741494181Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1836920489\n",
      "1591926120\n",
      "1001234244\n",
      "247748062\n",
      "167102995\n",
      "1966492551\n",
      "5270799\n",
      "-988021557\n",
      "-780350396\n",
      "-255809126\n"
     ]
    }
   ],
   "source": [
    "# get annotations that differ or both rejected\n",
    "for name in annotations2:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    if ann1[\"answer\"] == \"reject\" or ann2[\"answer\"] == \"reject\":\n",
    "        print(ann1[\"_input_hash\"])\n",
    "# 3 both rejected and 7 not same answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:58:14.144360203Z",
     "start_time": "2023-07-06T08:58:14.133517276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not accepted:  7 accepted:  46\n"
     ]
    }
   ],
   "source": [
    "not_accepted1 = []\n",
    "accepted1 = []\n",
    "for name, ann in annotations1.items():\n",
    "    if ann[\"answer\"] != \"accept\":\n",
    "        not_accepted1.append(name)\n",
    "    else:\n",
    "        accepted1.append(name)\n",
    "print(\"not accepted: \",len(not_accepted1), \"accepted: \", len(accepted1))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:38.902016364Z",
     "start_time": "2023-07-06T08:42:38.888043669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not accepted:  6 accepted:  47\n"
     ]
    }
   ],
   "source": [
    "not_accepted2 = []\n",
    "accepted2 = []\n",
    "for name, ann in annotations2.items():\n",
    "    if ann[\"answer\"] != \"accept\":\n",
    "        not_accepted2.append(name)\n",
    "    else:\n",
    "        accepted2.append(name)\n",
    "print(\"not accepted: \",len(not_accepted2), \"accepted: \", len(accepted2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:40.680586790Z",
     "start_time": "2023-07-06T08:42:40.658485480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both accepted:  43\n",
      "both discarded:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"both accepted: \", len(set(accepted1).intersection(set(accepted2))))\n",
    "print(\"both discarded: \", len(set(not_accepted1).intersection(set(not_accepted2))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:41.276739634Z",
     "start_time": "2023-07-06T08:42:41.263408754Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted1 but not accepted2:  3\n",
      "accepted2 but not accepted1:  4\n",
      "not_accepted1 but not_accepted2:  4\n",
      "not_accepted2 but not_accepted1:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"accepted1 but not accepted2: \", len(set(accepted1).difference(set(accepted2))))\n",
    "print(\"accepted2 but not accepted1: \", len(set(accepted2).difference(set(accepted1))))\n",
    "\n",
    "print(\"not_accepted1 but not_accepted2: \", len(set(not_accepted1).difference(set(not_accepted2))))\n",
    "print(\"not_accepted2 but not_accepted1: \", len(set(not_accepted2).difference(set(not_accepted1))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:52.626239763Z",
     "start_time": "2023-07-06T08:42:52.598787071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers:  reject - accept\n",
      "Einklemmung ->  <b>C1261287</b>: Stenose: Der Zustand, dass eine anatomische Struktur über die normalen Maße hinaus eingeengt ist.\n",
      "\n",
      "Answers:  reject - accept\n",
      "Auslöser ->  <b>C0032930</b>: Niederschlagende Faktoren: Faktoren, die mit dem endgültigen Ausbruch einer Krankheit, eines Unfalls, einer Verhaltensreaktion oder einer Handlung zusammenhängen. Wenn mehrere Faktoren beteiligt sind, ist in der Regel ein Faktor wichtiger oder offensichtlicher als andere, und einer kann oft als \"notwendig\" angesehen werden. Beispiele hierfür sind die Exposition gegenüber einer bestimmten Krankheit, die Menge oder das Ausmaß eines infektiösen Organismus, einer Droge oder einer schädlichen Substanz usw.\n",
      "\n",
      "Answers:  accept - reject\n",
      "Magen Darm Virus ->  <b>C4082764</b>: Gastrointestinale Infektion: Wiederkehrende Infektionen des Magen-Darm-Trakts. [HPO:probinson]\n",
      "\n",
      "Answers:  accept - reject\n",
      "Schilddrüse ist in Ordnung ->  <b>C0040132</b>: Schilddrüse: Eine stark vaskularisierte endokrine Drüse, die aus zwei Lappen besteht, die durch ein dünnes Gewebeband verbunden sind, wobei sich auf jeder Seite der TRACHEA ein Lappen befindet. Sie sondert Schilddrüsenhormone aus den Follikelzellen und KALZITONIN aus den parafollikulären Zellen ab und reguliert damit den Stoffwechsel und den KALZIUM-Spiegel im Blut.\n",
      "\n",
      "Answers:  reject - accept\n",
      "Ursache ->  <b>C0032930</b>: Niederschlagende Faktoren: Faktoren, die mit dem endgültigen Ausbruch einer Krankheit, eines Unfalls, einer Verhaltensreaktion oder einer Handlung zusammenhängen. Wenn mehrere Faktoren beteiligt sind, ist in der Regel ein Faktor wichtiger oder offensichtlicher als andere, und einer kann oft als \"notwendig\" angesehen werden. Beispiele hierfür sind die Exposition gegenüber einer bestimmten Krankheit, die Menge oder das Ausmaß eines infektiösen Organismus, einer Droge oder einer schädlichen Substanz usw.\n",
      "\n",
      "Answers:  reject - accept\n",
      "nach dem Essen ->  <b>C0376674</b>: Postprandiale Phase: Die Zeitspanne nach einer Mahlzeit oder Nahrungsaufnahme.\n",
      "\n",
      "Answers:  accept - reject\n",
      "harntreibend ->  <b>C0012798</b>: Diuretika: Wirkstoffe, die durch ihre Wirkung auf die Nierenfunktion die Ausscheidung von Urin fördern.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in diff:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    print(\"Answers: \", ann1[\"answer\"],\"-\", ann2[\"answer\"])\n",
    "    start = ann1[\"spans\"][0][\"start\"]\n",
    "    end = ann1[\"spans\"][0][\"end\"]\n",
    "    print(ann1[\"text\"][start:end], \"-> \", ann1['html'])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:41:42.115748392Z",
     "start_time": "2023-07-06T08:41:42.102699343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "diff1 = [annotations1[name] for name in diff]\n",
    "diff2 = [annotations2[name] for name in diff]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:01.415142531Z",
     "start_time": "2023-07-06T08:42:01.408382748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'Thread: [Einklemmung/Darmverschluss durch oder was anders? Ärzte ratlos]\\nText: [Auch wenn ein Osteophat keine Verwachsungen fühlen konnte, könnten es Verwachsungsbeschwerden sein.\\nDas ist oft unglaublich, wieviele und heftige Beschwerden Verwachsungen machen können.]\\n',\n 'spans': [{'start': 9, 'end': 20, 'label': 'Mention'}],\n 'html': '<b>C1261287</b>: Stenose: Der Zustand, dass eine anatomische Struktur über die normalen Maße hinaus eingeengt ist.',\n 'annotation_ids': [5652,\n  6688,\n  6492,\n  5196,\n  5563,\n  5017,\n  5330,\n  6388,\n  5037,\n  4855,\n  4985,\n  6987,\n  5075],\n 'cui': 'C1261287',\n 'meta': {'url': 'https://uts.nlm.nih.gov/uts/umls/concept/C1261287'},\n '_input_hash': 1591926120,\n '_task_hash': -1718244887,\n '_view_id': 'blocks',\n 'answer': 'reject',\n '_timestamp': 1687772550,\n '_annotator_id': 'TLC-validation-second-round-alon',\n '_session_id': 'TLC-validation-second-round-alon'}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff1[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:42:01.802976665Z",
     "start_time": "2023-07-06T08:42:01.758934944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "annotator agreement\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3867768595041322"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate kappa score given that we treat ignore as same with the other answer and treat both ignore as both rejected\n",
    "a1, a2 = [], []\n",
    "for name in annotations1:\n",
    "    ann1 = annotations1[name]\n",
    "    ann2 = annotations2[name]\n",
    "    a1.append(ann1[\"answer\"])\n",
    "    a2.append(ann2[\"answer\"])\n",
    "    \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(a1, a2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:40:27.619519340Z",
     "start_time": "2023-07-06T08:40:26.656761656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      accept       0.91      0.93      0.92        46\n",
      "      reject       0.50      0.43      0.46         7\n",
      "\n",
      "    accuracy                           0.87        53\n",
      "   macro avg       0.71      0.68      0.69        53\n",
      "weighted avg       0.86      0.87      0.86        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(a1, a2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:40:28.959949595Z",
     "start_time": "2023-07-06T08:40:28.933663273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      accept       0.89      0.91      0.90       574\n",
      "      reject       0.79      0.75      0.77       258\n",
      "\n",
      "    accuracy                           0.86       832\n",
      "   macro avg       0.84      0.83      0.84       832\n",
      "weighted avg       0.86      0.86      0.86       832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(a2, a1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T12:10:31.383974846Z",
     "start_time": "2023-06-23T12:10:31.344164959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Latex Table from Classification Report}\n",
      "\\label{table:classification:report}\n",
      "\\centering\n",
      "\\begin{tabular}{c | c c c r}\n",
      "Class & Precision & Recall & F-score & Support\\\\\n",
      "\\midrule\n",
      "accept & 0.91 & 0.89 & 0.9 & 587\\\\\n",
      "reject & 0.75 & 0.79 & 0.77 & 245\\\\\n",
      "macro avg & 0.83 & 0.84 & 0.84 & 832\\\\\n",
      "avg & 0.86 & 0.86 & 0.86 & 832\\\\\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "def parse_classification_report(clfreport):\n",
    "    \"\"\"\n",
    "    Parse a sklearn classification report into a dict keyed by class name\n",
    "    and containing a tuple (precision, recall, fscore, support) for each class\n",
    "    \"\"\"\n",
    "    lines = clfreport.split('\\n')\n",
    "    # Remove empty lines\n",
    "    lines = list(filter(lambda l: not len(l.strip()) == 0, lines))\n",
    "    lines=[s for s in lines if 'accuracy' not in s]\n",
    "\n",
    "    # Starts with a header, then score for each class and finally an average\n",
    "    header = lines[0]\n",
    "    cls_lines = lines[1:-1]\n",
    "    avg_line = lines[-1]\n",
    "\n",
    "    assert header.split() == ['precision', 'recall', 'f1-score', 'support']\n",
    "    assert avg_line.split()[1] == 'avg'\n",
    "\n",
    "    # We cannot simply use split because class names can have spaces. So instead\n",
    "    # figure the width of the class field by looking at the indentation of the\n",
    "    # precision header\n",
    "    cls_field_width = len(header) - len(header.lstrip())\n",
    "    # Now, collect all the class names and score in a dict\n",
    "    def parse_line(l):\n",
    "        \"\"\"Parse a line of classification_report\"\"\"\n",
    "        cls_name = l[:cls_field_width].strip()\n",
    "        precision, recall, fscore, support = l[cls_field_width:].split()\n",
    "        precision = float(precision)\n",
    "        recall = float(recall)\n",
    "        fscore = float(fscore)\n",
    "        support = int(support)\n",
    "        return (cls_name, precision, recall, fscore, support)\n",
    "\n",
    "    data = collections.OrderedDict()\n",
    "    for l in cls_lines:\n",
    "        ret = parse_line(l)\n",
    "        cls_name = ret[0]\n",
    "        scores = ret[1:]\n",
    "        data[cls_name] = scores\n",
    "\n",
    "    # average\n",
    "    data['avg'] = parse_line(avg_line)[1:]\n",
    "\n",
    "    return data\n",
    "\n",
    "def report_to_latex_table(data):\n",
    "    avg_split = False\n",
    "    out = \"\"\n",
    "    out += \"\\\\begin{table}\\n\"\n",
    "    out += \"\\\\caption{Latex Table from Classification Report}\\n\"\n",
    "    out += \"\\\\label{table:classification:report}\\n\"\n",
    "    out += \"\\\\centering\\n\"\n",
    "    out += \"\\\\begin{tabular}{c | c c c r}\\n\"\n",
    "    out += \"Class & Precision & Recall & F-score & Support\\\\\\\\\\n\"\n",
    "    out += \"\\midrule\\n\"\n",
    "    for cls, scores in data.items():\n",
    "        if 'micro' in cls:\n",
    "            out += \"\\\\midrule\\n\"\n",
    "        out += cls + \" & \" + \" & \".join([str(s) for s in scores])\n",
    "        out += \"\\\\\\\\\\n\"\n",
    "    out += \"\\\\end{tabular}\\n\"\n",
    "    out += \"\\\\end{table}\"\n",
    "    return out\n",
    "\n",
    "print(report_to_latex_table(parse_classification_report(classification_report(a1, a2))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T12:16:56.832598560Z",
     "start_time": "2023-06-23T12:16:56.776698577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "'              precision    recall  f1-score   support\\n\\n      accept       0.91      0.89      0.90       587\\n      reject       0.75      0.79      0.77       245\\n\\n    accuracy                           0.86       832\\n   macro avg       0.83      0.84      0.84       832\\nweighted avg       0.86      0.86      0.86       832\\n'"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T12:14:52.257098188Z",
     "start_time": "2023-06-23T12:14:52.218331907Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
